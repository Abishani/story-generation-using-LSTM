{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Idea – Story Generation using AI</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4>Part A - Application Area Review</h4>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Citation\n",
    "   </td>\n",
    "   <td>Summary\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Story Scrambler - Automatic Text Generation Using Word Level RNN-LSTM\n",
    "<p>\n",
    "<strong>(Assistant Professor, K. J. Somaiya College of Engineering, Department of IT, Mumbai et al., 2018)</strong>\n",
    "   </td>\n",
    "   <td>In this paper they have implemented text generation using LSTM based RNN to generate a story by inputting multiple stories. Here the stories are taken using different storylines and characters and stories with same storylines and characters. In this research to reduce the loss function SGD (Stochastic Gradient Descent optimizer) is used. Here unsupervised learning is used.\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>A Survey of Deep Learning Applied to Story Generation\n",
    "<p>\n",
    "<strong>(Hou et al., 2019)</strong>\n",
    "   </td>\n",
    "   <td>Basically, for the story generation this seq2seq will be applied. Seq2seq models are widely used for story generation but it has some drawbacks like difficulty in generating a story with long text, coherency and clear story. Story generation methods are divided into 3 categories: theme-based, storyline based and human-machine interaction. Story generation can be applied to various fields like education, entertainment and writing suggestions.\n",
    "<p>\n",
    "Discusses about the story generation models advantage and disadvantages based on coherence, consistency, logical reasoning and vividness\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Story Generation from Sequence of Independent Short Descriptions\n",
    "<p>\n",
    "<strong>(Jain et al., 2017)</strong>\n",
    "   </td>\n",
    "   <td>In this research, generated the story using short descriptions using sequence-to-sequence recurrent neural network. Used the bidirectional encoder to separate the independent descriptions and use decoder to generate the story sequence.\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Hierarchical Neural Story Generation\n",
    "<p>\n",
    "<strong>(Fan, Lewis and Dauphin, 2018)</strong>\n",
    "   </td>\n",
    "   <td>In this research paper generated the story by giving the prompt as an input and used the hierarchical model.\n",
    "<p>\n",
    "To improve the relevancy between the generated story and prompt here they have used the fusion mechanism and it is trained with the seq2seq model. Using a convolutional architecture improved the efficiency. for the tokenization NLTK is used.\n",
    "<p>\n",
    "In this research random sampling method is used to generate the next word. This is more effective than beam search. This fusion mechanism has return highest prompt ranking when comparing to convolutional seq2seq, Conv seq2seq + self-attention, Ensemble model.\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Story generation is a challenging problem in the Natural Language Processing (NLP) field. This problem domain is part of text generation. Seq2seq model is the basic model which is used for story generation **(Hou et al., 2019)**.  Hierarchical neural story generation model generated more coherent stories by using fusion mechanism **(Fan, Lewis and Dauphin, 2018)**. Transformers like BERT and GPT2 models give outstanding performances in this field and they have the ability to generate coherent long texts **(Zimmerman, Sahu and Vechtomova, 2022)**. Novel storytelling generation framework has been implemented using Latent variable model **(Tang and Wu, 2021)** story generated by this model is more clear and sensible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Part B – Compare and evaluate AI techniques</h4>\n",
    "\n",
    "The AI techniques which applied in story generation problem are BERT, LSTM and GPT-3. BERT and GPT-3 are transformer models and LSTM is a type of RNN algorithm. \n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>AI technique</strong>\n",
    "   </td>\n",
    "   <td>BERT\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Strengths</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Pretrained on large corpus of text data, which allows it to understand the context of the input text well.\n",
    "\n",
    "<li>Easy to use this model for smaller and more defined NLP tasks.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Weakness</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Struggle with understand the input text which is not similar to the text it was pre-trained on.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Advantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Useful for multilingual projects because it has pretrained in more than 100 languages\n",
    "\n",
    "<li>Suitable for language understanding, question answering and text classification tasks.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Disadvantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Can be computationally expensive and memory intensive to fine tune for specific tasks.\n",
    "\n",
    "<li>Slow to train the model.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Examples</strong>\n",
    "   </td>\n",
    "   <td>This BERT technique can be applied to story generation problem in 2 ways\n",
    "<ul>\n",
    "\n",
    "<li>Language modelling: by understanding the input word’s context BERT model generate accurate and coherent story. \n",
    "\n",
    "<li>Text classification: classify the input text into different genres of story, then generating the story based on that. \n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Input Method</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>In a text generation task Text data such as sentence or word\n",
    "\n",
    "<li>In a summarization task story or paragraph\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Output Method</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>In a text generation output would be generated paragraph based on the input word or sentence.\n",
    "\n",
    "<li>In a summarization task output would be summary of that story or paragraph.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>AI techniques</strong>\n",
    "   </td>\n",
    "   <td>RNN-LSTM\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Strengths</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Good at handling sequential data, particularly when there are long-term dependencies in the data.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Weakness</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Struggle with very large input sequenced data\n",
    "\n",
    "<li>Not suitable for prediction or classification task where the input data is not sequence.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Advantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Suitable for language modelling, text generation and speech recognition\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Disadvantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Not suitable to work with high nonlinear and lot of noisy data\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Examples</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Language modelling: predicting the next word in a sentence based on the previous words in the sentence.\n",
    "\n",
    "<li>Text generation: generate new text that is similar to a given input text\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Input Method</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>In the language modelling task, the input would be a sequence of words \n",
    "\n",
    "<li>In a time-series forecasting task, the input would be a sequence of data points\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Output Method</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>In the language modelling task the output would be the next word in that sequence\n",
    "\n",
    "<li>In a time-series forecasting task, output would be a forecast of future data points\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>AI techniques</strong>\n",
    "   </td>\n",
    "   <td>GPT-3\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Strengths</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>High powerful model.\n",
    "\n",
    "<li>Can generate large amount of text\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Weakness</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Output depends on the trained data and lack of creativity.\n",
    "\n",
    "<li>Making the text boring and monotonous\n",
    "\n",
    "<li>It is a pretrained model so it does not have an ongoing long-term memory\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Advantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Can generate high quality human-like text that is semantically and syntactically coherent.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Disadvantages</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Not understanding real world sensibility and coherence\n",
    "\n",
    "<li>Need large amount of data.\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Examples</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "<ul>\n",
    "\n",
    "<li>Question answering: GPT-3 can be used as a QA chatbot. It will generate answers based on the text which is pre-trained.\n",
    "\n",
    "<li>Generating text summary: It will summarize the long sentence or paragraph\n",
    "\n",
    "<li>Generating the code: real world example for this is GitHub code pilot it is uses GPT-3 model\n",
    "</li>\n",
    "</ul>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Input Method</strong>\n",
    "   </td>\n",
    "   <td>In text generation input type would be word or sentence. \n",
    "<p>\n",
    "In story generation input would be text or sentence.\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Output Method</strong>\n",
    "   </td>\n",
    "   <td>In text generation output type would be paragraph generated according to the given word or sentence. \n",
    "<p>\n",
    "In story generation with images output would be images (eg: DALL-E)\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "From these 3 techniques I have selected the LSTM model when compared to other 2 models LSTM is good in handling sequential data and it is suitable where the data has long-term dependencies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Part C – Implementation</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. Description about dataset</h5>\n",
    "\n",
    "Story Generation implementation I need text data for that system needs to train using the story dataset. Here I have used dataset contains folk stories and these stories are translations of German folk story books to English by the Brothers Grimm. From this dataset I have used first 100 books for the implementation. This dataset can be access through this website [https://www.cs.cmu.edu/~spok/grimmtmp/](https://www.cs.cmu.edu/~spok/grimmtmp/). Here I have implemented three preprocessing steps for the dataset. first lowercasing the text; in this step, all the text was changed to lowercase. Then I implemented the tokenization. In tokenization, the text available in each story is broken down into character n-grams (n = 2 or bigrams) to generate more realistic and coherent text. At last to reduce the vocabulary size all the bigrams which is displayed fewer than 10 times in the corpus has been tokenized as UNK (Unknown)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b. High Level Diagram</h4>\n",
    "\n",
    "<img src = \"HighLevelDiagram.png\" width=\"80%\" title=\"High level Diagram\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>c. working example prototype</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution() # code added to diable tensorflow's eager execution\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Stories\n",
    "Stories are automatically downloaded from https://www.cs.cmu.edu/~spok/grimmtmp/, if not detected in the disk. The total size of stories is around ~500KB. The dataset consists of 100 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories\\001.txt\n",
      "File  001.txt  already exists.\n",
      "Downloading file:  stories\\002.txt\n",
      "File  002.txt  already exists.\n",
      "Downloading file:  stories\\003.txt\n",
      "File  003.txt  already exists.\n",
      "Downloading file:  stories\\004.txt\n",
      "File  004.txt  already exists.\n",
      "Downloading file:  stories\\005.txt\n",
      "File  005.txt  already exists.\n",
      "Downloading file:  stories\\006.txt\n",
      "File  006.txt  already exists.\n",
      "Downloading file:  stories\\007.txt\n",
      "File  007.txt  already exists.\n",
      "Downloading file:  stories\\008.txt\n",
      "File  008.txt  already exists.\n",
      "Downloading file:  stories\\009.txt\n",
      "File  009.txt  already exists.\n",
      "Downloading file:  stories\\010.txt\n",
      "File  010.txt  already exists.\n",
      "Downloading file:  stories\\011.txt\n",
      "File  011.txt  already exists.\n",
      "Downloading file:  stories\\012.txt\n",
      "File  012.txt  already exists.\n",
      "Downloading file:  stories\\013.txt\n",
      "File  013.txt  already exists.\n",
      "Downloading file:  stories\\014.txt\n",
      "File  014.txt  already exists.\n",
      "Downloading file:  stories\\015.txt\n",
      "File  015.txt  already exists.\n",
      "Downloading file:  stories\\016.txt\n",
      "File  016.txt  already exists.\n",
      "Downloading file:  stories\\017.txt\n",
      "File  017.txt  already exists.\n",
      "Downloading file:  stories\\018.txt\n",
      "File  018.txt  already exists.\n",
      "Downloading file:  stories\\019.txt\n",
      "File  019.txt  already exists.\n",
      "Downloading file:  stories\\020.txt\n",
      "File  020.txt  already exists.\n",
      "Downloading file:  stories\\021.txt\n",
      "File  021.txt  already exists.\n",
      "Downloading file:  stories\\022.txt\n",
      "File  022.txt  already exists.\n",
      "Downloading file:  stories\\023.txt\n",
      "File  023.txt  already exists.\n",
      "Downloading file:  stories\\024.txt\n",
      "File  024.txt  already exists.\n",
      "Downloading file:  stories\\025.txt\n",
      "File  025.txt  already exists.\n",
      "Downloading file:  stories\\026.txt\n",
      "File  026.txt  already exists.\n",
      "Downloading file:  stories\\027.txt\n",
      "File  027.txt  already exists.\n",
      "Downloading file:  stories\\028.txt\n",
      "File  028.txt  already exists.\n",
      "Downloading file:  stories\\029.txt\n",
      "File  029.txt  already exists.\n",
      "Downloading file:  stories\\030.txt\n",
      "File  030.txt  already exists.\n",
      "Downloading file:  stories\\031.txt\n",
      "File  031.txt  already exists.\n",
      "Downloading file:  stories\\032.txt\n",
      "File  032.txt  already exists.\n",
      "Downloading file:  stories\\033.txt\n",
      "File  033.txt  already exists.\n",
      "Downloading file:  stories\\034.txt\n",
      "File  034.txt  already exists.\n",
      "Downloading file:  stories\\035.txt\n",
      "File  035.txt  already exists.\n",
      "Downloading file:  stories\\036.txt\n",
      "File  036.txt  already exists.\n",
      "Downloading file:  stories\\037.txt\n",
      "File  037.txt  already exists.\n",
      "Downloading file:  stories\\038.txt\n",
      "File  038.txt  already exists.\n",
      "Downloading file:  stories\\039.txt\n",
      "File  039.txt  already exists.\n",
      "Downloading file:  stories\\040.txt\n",
      "File  040.txt  already exists.\n",
      "Downloading file:  stories\\041.txt\n",
      "File  041.txt  already exists.\n",
      "Downloading file:  stories\\042.txt\n",
      "File  042.txt  already exists.\n",
      "Downloading file:  stories\\043.txt\n",
      "File  043.txt  already exists.\n",
      "Downloading file:  stories\\044.txt\n",
      "File  044.txt  already exists.\n",
      "Downloading file:  stories\\045.txt\n",
      "File  045.txt  already exists.\n",
      "Downloading file:  stories\\046.txt\n",
      "File  046.txt  already exists.\n",
      "Downloading file:  stories\\047.txt\n",
      "File  047.txt  already exists.\n",
      "Downloading file:  stories\\048.txt\n",
      "File  048.txt  already exists.\n",
      "Downloading file:  stories\\049.txt\n",
      "File  049.txt  already exists.\n",
      "Downloading file:  stories\\050.txt\n",
      "File  050.txt  already exists.\n",
      "Downloading file:  stories\\051.txt\n",
      "File  051.txt  already exists.\n",
      "Downloading file:  stories\\052.txt\n",
      "File  052.txt  already exists.\n",
      "Downloading file:  stories\\053.txt\n",
      "File  053.txt  already exists.\n",
      "Downloading file:  stories\\054.txt\n",
      "File  054.txt  already exists.\n",
      "Downloading file:  stories\\055.txt\n",
      "File  055.txt  already exists.\n",
      "Downloading file:  stories\\056.txt\n",
      "File  056.txt  already exists.\n",
      "Downloading file:  stories\\057.txt\n",
      "File  057.txt  already exists.\n",
      "Downloading file:  stories\\058.txt\n",
      "File  058.txt  already exists.\n",
      "Downloading file:  stories\\059.txt\n",
      "File  059.txt  already exists.\n",
      "Downloading file:  stories\\060.txt\n",
      "File  060.txt  already exists.\n",
      "Downloading file:  stories\\061.txt\n",
      "File  061.txt  already exists.\n",
      "Downloading file:  stories\\062.txt\n",
      "File  062.txt  already exists.\n",
      "Downloading file:  stories\\063.txt\n",
      "File  063.txt  already exists.\n",
      "Downloading file:  stories\\064.txt\n",
      "File  064.txt  already exists.\n",
      "Downloading file:  stories\\065.txt\n",
      "File  065.txt  already exists.\n",
      "Downloading file:  stories\\066.txt\n",
      "File  066.txt  already exists.\n",
      "Downloading file:  stories\\067.txt\n",
      "File  067.txt  already exists.\n",
      "Downloading file:  stories\\068.txt\n",
      "File  068.txt  already exists.\n",
      "Downloading file:  stories\\069.txt\n",
      "File  069.txt  already exists.\n",
      "Downloading file:  stories\\070.txt\n",
      "File  070.txt  already exists.\n",
      "Downloading file:  stories\\071.txt\n",
      "File  071.txt  already exists.\n",
      "Downloading file:  stories\\072.txt\n",
      "File  072.txt  already exists.\n",
      "Downloading file:  stories\\073.txt\n",
      "File  073.txt  already exists.\n",
      "Downloading file:  stories\\074.txt\n",
      "File  074.txt  already exists.\n",
      "Downloading file:  stories\\075.txt\n",
      "File  075.txt  already exists.\n",
      "Downloading file:  stories\\076.txt\n",
      "File  076.txt  already exists.\n",
      "Downloading file:  stories\\077.txt\n",
      "File  077.txt  already exists.\n",
      "Downloading file:  stories\\078.txt\n",
      "File  078.txt  already exists.\n",
      "Downloading file:  stories\\079.txt\n",
      "File  079.txt  already exists.\n",
      "Downloading file:  stories\\080.txt\n",
      "File  080.txt  already exists.\n",
      "Downloading file:  stories\\081.txt\n",
      "File  081.txt  already exists.\n",
      "Downloading file:  stories\\082.txt\n",
      "File  082.txt  already exists.\n",
      "Downloading file:  stories\\083.txt\n",
      "File  083.txt  already exists.\n",
      "Downloading file:  stories\\084.txt\n",
      "File  084.txt  already exists.\n",
      "Downloading file:  stories\\085.txt\n",
      "File  085.txt  already exists.\n",
      "Downloading file:  stories\\086.txt\n",
      "File  086.txt  already exists.\n",
      "Downloading file:  stories\\087.txt\n",
      "File  087.txt  already exists.\n",
      "Downloading file:  stories\\088.txt\n",
      "File  088.txt  already exists.\n",
      "Downloading file:  stories\\089.txt\n",
      "File  089.txt  already exists.\n",
      "Downloading file:  stories\\090.txt\n",
      "File  090.txt  already exists.\n",
      "Downloading file:  stories\\091.txt\n",
      "File  091.txt  already exists.\n",
      "Downloading file:  stories\\092.txt\n",
      "File  092.txt  already exists.\n",
      "Downloading file:  stories\\093.txt\n",
      "File  093.txt  already exists.\n",
      "Downloading file:  stories\\094.txt\n",
      "File  094.txt  already exists.\n",
      "Downloading file:  stories\\095.txt\n",
      "File  095.txt  already exists.\n",
      "Downloading file:  stories\\096.txt\n",
      "File  096.txt  already exists.\n",
      "Downloading file:  stories\\097.txt\n",
      "File  097.txt  already exists.\n",
      "Downloading file:  stories\\098.txt\n",
      "File  098.txt  already exists.\n",
      "Downloading file:  stories\\099.txt\n",
      "File  099.txt  already exists.\n",
      "Downloading file:  stories\\100.txt\n",
      "File  100.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "\n",
    "# Create a directory if needed\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download a file if not present\"\"\"\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "num_files = 100\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.'%len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "Data will be stored in a list of lists where the each list represents a document and document is a list of words. We will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file stories\\001.txt\n",
      "Data size (Characters) (Document 0) 3667\n",
      "Sample string (Document 0) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
      "\n",
      "Processing file stories\\002.txt\n",
      "Data size (Characters) (Document 1) 4928\n",
      "Sample string (Document 1) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
      "\n",
      "Processing file stories\\003.txt\n",
      "Data size (Characters) (Document 2) 9745\n",
      "Sample string (Document 2) ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
      "\n",
      "Processing file stories\\004.txt\n",
      "Data size (Characters) (Document 3) 2852\n",
      "Sample string (Document 3) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
      "\n",
      "Processing file stories\\005.txt\n",
      "Data size (Characters) (Document 4) 8189\n",
      "Sample string (Document 4) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
      "\n",
      "Processing file stories\\006.txt\n",
      "Data size (Characters) (Document 5) 4369\n",
      "Sample string (Document 5) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "Processing file stories\\007.txt\n",
      "Data size (Characters) (Document 6) 5216\n",
      "Sample string (Document 6) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
      "\n",
      "Processing file stories\\008.txt\n",
      "Data size (Characters) (Document 7) 6097\n",
      "Sample string (Document 7) ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
      "\n",
      "Processing file stories\\009.txt\n",
      "Data size (Characters) (Document 8) 3699\n",
      "Sample string (Document 8) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
      "\n",
      "Processing file stories\\010.txt\n",
      "Data size (Characters) (Document 9) 5268\n",
      "Sample string (Document 9) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories\\011.txt\n",
      "Data size (Characters) (Document 10) 2377\n",
      "Sample string (Document 10) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
      "\n",
      "Processing file stories\\012.txt\n",
      "Data size (Characters) (Document 11) 7695\n",
      "Sample string (Document 11) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
      "\n",
      "Processing file stories\\013.txt\n",
      "Data size (Characters) (Document 12) 3665\n",
      "Sample string (Document 12) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
      "\n",
      "Processing file stories\\014.txt\n",
      "Data size (Characters) (Document 13) 4178\n",
      "Sample string (Document 13) ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
      "\n",
      "Processing file stories\\015.txt\n",
      "Data size (Characters) (Document 14) 8674\n",
      "Sample string (Document 14) ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
      "\n",
      "Processing file stories\\016.txt\n",
      "Data size (Characters) (Document 15) 7018\n",
      "Sample string (Document 15) ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
      "\n",
      "Processing file stories\\017.txt\n",
      "Data size (Characters) (Document 16) 3039\n",
      "Sample string (Document 16) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
      "\n",
      "Processing file stories\\018.txt\n",
      "Data size (Characters) (Document 17) 3020\n",
      "Sample string (Document 17) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
      "\n",
      "Processing file stories\\019.txt\n",
      "Data size (Characters) (Document 18) 2465\n",
      "Sample string (Document 18) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
      "\n",
      "Processing file stories\\020.txt\n",
      "Data size (Characters) (Document 19) 3703\n",
      "Sample string (Document 19) ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
      "\n",
      "Processing file stories\\021.txt\n",
      "Data size (Characters) (Document 20) 1924\n",
      "Sample string (Document 20) ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
      "\n",
      "Processing file stories\\022.txt\n",
      "Data size (Characters) (Document 21) 6561\n",
      "Sample string (Document 21) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
      "\n",
      "Processing file stories\\023.txt\n",
      "Data size (Characters) (Document 22) 5956\n",
      "Sample string (Document 22) ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
      "\n",
      "Processing file stories\\024.txt\n",
      "Data size (Characters) (Document 23) 2529\n",
      "Sample string (Document 23) ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
      "\n",
      "Processing file stories\\025.txt\n",
      "Data size (Characters) (Document 24) 2416\n",
      "Sample string (Document 24) ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
      "\n",
      "Processing file stories\\026.txt\n",
      "Data size (Characters) (Document 25) 3369\n",
      "Sample string (Document 25) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
      "\n",
      "Processing file stories\\027.txt\n",
      "Data size (Characters) (Document 26) 10013\n",
      "Sample string (Document 26) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
      "\n",
      "Processing file stories\\028.txt\n",
      "Data size (Characters) (Document 27) 5788\n",
      "Sample string (Document 27) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
      "\n",
      "Processing file stories\\029.txt\n",
      "Data size (Characters) (Document 28) 1335\n",
      "Sample string (Document 28) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
      "\n",
      "Processing file stories\\030.txt\n",
      "Data size (Characters) (Document 29) 3591\n",
      "Sample string (Document 29) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
      "\n",
      "Processing file stories\\031.txt\n",
      "Data size (Characters) (Document 30) 1624\n",
      "Sample string (Document 30) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
      "\n",
      "Processing file stories\\032.txt\n",
      "Data size (Characters) (Document 31) 758\n",
      "Sample string (Document 31) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
      "\n",
      "Processing file stories\\033.txt\n",
      "Data size (Characters) (Document 32) 3121\n",
      "Sample string (Document 32) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
      "\n",
      "Processing file stories\\034.txt\n",
      "Data size (Characters) (Document 33) 4192\n",
      "Sample string (Document 33) ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
      "\n",
      "Processing file stories\\035.txt\n",
      "Data size (Characters) (Document 34) 3650\n",
      "Sample string (Document 34) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
      "\n",
      "Processing file stories\\036.txt\n",
      "Data size (Characters) (Document 35) 8219\n",
      "Sample string (Document 35) ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
      "\n",
      "Processing file stories\\037.txt\n",
      "Data size (Characters) (Document 36) 2151\n",
      "Sample string (Document 36) ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
      "\n",
      "Processing file stories\\038.txt\n",
      "Data size (Characters) (Document 37) 5129\n",
      "Sample string (Document 37) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
      "\n",
      "Processing file stories\\039.txt\n",
      "Data size (Characters) (Document 38) 3472\n",
      "Sample string (Document 38) ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
      "\n",
      "Processing file stories\\040.txt\n",
      "Data size (Characters) (Document 39) 2490\n",
      "Sample string (Document 39) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
      "\n",
      "Processing file stories\\041.txt\n",
      "Data size (Characters) (Document 40) 4273\n",
      "Sample string (Document 40) ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
      "\n",
      "Processing file stories\\042.txt\n",
      "Data size (Characters) (Document 41) 8327\n",
      "Sample string (Document 41) ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
      "\n",
      "Processing file stories\\043.txt\n",
      "Data size (Characters) (Document 42) 6128\n",
      "Sample string (Document 42) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
      "\n",
      "Processing file stories\\044.txt\n",
      "Data size (Characters) (Document 43) 2819\n",
      "Sample string (Document 43) ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
      "\n",
      "Processing file stories\\045.txt\n",
      "Data size (Characters) (Document 44) 3822\n",
      "Sample string (Document 44) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
      "\n",
      "Processing file stories\\046.txt\n",
      "Data size (Characters) (Document 45) 7772\n",
      "Sample string (Document 45) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
      "\n",
      "Processing file stories\\047.txt\n",
      "Data size (Characters) (Document 46) 22158\n",
      "Sample string (Document 46) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
      "\n",
      "Processing file stories\\048.txt\n",
      "Data size (Characters) (Document 47) 2169\n",
      "Sample string (Document 47) ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
      "\n",
      "Processing file stories\\049.txt\n",
      "Data size (Characters) (Document 48) 2822\n",
      "Sample string (Document 48) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
      "\n",
      "Processing file stories\\050.txt\n",
      "Data size (Characters) (Document 49) 4034\n",
      "Sample string (Document 49) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
      "\n",
      "Processing file stories\\051.txt\n",
      "Data size (Characters) (Document 50) 5608\n",
      "Sample string (Document 50) ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
      "\n",
      "Processing file stories\\052.txt\n",
      "Data size (Characters) (Document 51) 1287\n",
      "Sample string (Document 51) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
      "\n",
      "Processing file stories\\053.txt\n",
      "Data size (Characters) (Document 52) 2841\n",
      "Sample string (Document 52) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
      "\n",
      "Processing file stories\\054.txt\n",
      "Data size (Characters) (Document 53) 1922\n",
      "Sample string (Document 53) ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
      "\n",
      "Processing file stories\\055.txt\n",
      "Data size (Characters) (Document 54) 2573\n",
      "Sample string (Document 54) ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
      "\n",
      "Processing file stories\\056.txt\n",
      "Data size (Characters) (Document 55) 5285\n",
      "Sample string (Document 55) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
      "\n",
      "Processing file stories\\057.txt\n",
      "Data size (Characters) (Document 56) 971\n",
      "Sample string (Document 56) ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
      "\n",
      "Processing file stories\\058.txt\n",
      "Data size (Characters) (Document 57) 4538\n",
      "Sample string (Document 57) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
      "\n",
      "Processing file stories\\059.txt\n",
      "Data size (Characters) (Document 58) 636\n",
      "Sample string (Document 58) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
      "\n",
      "Processing file stories\\060.txt\n",
      "Data size (Characters) (Document 59) 786\n",
      "Sample string (Document 59) ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
      "\n",
      "Processing file stories\\061.txt\n",
      "Data size (Characters) (Document 60) 10687\n",
      "Sample string (Document 60) ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
      "\n",
      "Processing file stories\\062.txt\n",
      "Data size (Characters) (Document 61) 5105\n",
      "Sample string (Document 61) ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
      "\n",
      "Processing file stories\\063.txt\n",
      "Data size (Characters) (Document 62) 1127\n",
      "Sample string (Document 62) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
      "\n",
      "Processing file stories\\064.txt\n",
      "Data size (Characters) (Document 63) 4981\n",
      "Sample string (Document 63) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
      "\n",
      "Processing file stories\\065.txt\n",
      "Data size (Characters) (Document 64) 6006\n",
      "Sample string (Document 64) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
      "\n",
      "Processing file stories\\066.txt\n",
      "Data size (Characters) (Document 65) 5900\n",
      "Sample string (Document 65) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
      "\n",
      "Processing file stories\\067.txt\n",
      "Data size (Characters) (Document 66) 7837\n",
      "Sample string (Document 66) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories\\068.txt\n",
      "Data size (Characters) (Document 67) 4717\n",
      "Sample string (Document 67) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
      "\n",
      "Processing file stories\\069.txt\n",
      "Data size (Characters) (Document 68) 6233\n",
      "Sample string (Document 68) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
      "\n",
      "Processing file stories\\070.txt\n",
      "Data size (Characters) (Document 69) 5664\n",
      "Sample string (Document 69) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
      "\n",
      "Processing file stories\\071.txt\n",
      "Data size (Characters) (Document 70) 3569\n",
      "Sample string (Document 70) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
      "\n",
      "Processing file stories\\072.txt\n",
      "Data size (Characters) (Document 71) 3793\n",
      "Sample string (Document 71) ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
      "\n",
      "Processing file stories\\073.txt\n",
      "Data size (Characters) (Document 72) 5980\n",
      "Sample string (Document 72) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
      "\n",
      "Processing file stories\\074.txt\n",
      "Data size (Characters) (Document 73) 4518\n",
      "Sample string (Document 73) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
      "\n",
      "Processing file stories\\075.txt\n",
      "Data size (Characters) (Document 74) 3247\n",
      "Sample string (Document 74) ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
      "\n",
      "Processing file stories\\076.txt\n",
      "Data size (Characters) (Document 75) 5130\n",
      "Sample string (Document 75) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
      "\n",
      "Processing file stories\\077.txt\n",
      "Data size (Characters) (Document 76) 2401\n",
      "Sample string (Document 76) ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
      "\n",
      "Processing file stories\\078.txt\n",
      "Data size (Characters) (Document 77) 624\n",
      "Sample string (Document 77) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
      "\n",
      "Processing file stories\\079.txt\n",
      "Data size (Characters) (Document 78) 3991\n",
      "Sample string (Document 78) ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
      "\n",
      "Processing file stories\\080.txt\n",
      "Data size (Characters) (Document 79) 1426\n",
      "Sample string (Document 79) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
      "\n",
      "Processing file stories\\081.txt\n",
      "Data size (Characters) (Document 80) 3574\n",
      "Sample string (Document 80) ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
      "\n",
      "Processing file stories\\082.txt\n",
      "Data size (Characters) (Document 81) 10822\n",
      "Sample string (Document 81) ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
      "\n",
      "Processing file stories\\083.txt\n",
      "Data size (Characters) (Document 82) 5480\n",
      "Sample string (Document 82) ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
      "\n",
      "Processing file stories\\084.txt\n",
      "Data size (Characters) (Document 83) 658\n",
      "Sample string (Document 83) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
      "\n",
      "Processing file stories\\085.txt\n",
      "Data size (Characters) (Document 84) 5989\n",
      "Sample string (Document 84) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "Processing file stories\\086.txt\n",
      "Data size (Characters) (Document 85) 8758\n",
      "Sample string (Document 85) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
      "\n",
      "Processing file stories\\087.txt\n",
      "Data size (Characters) (Document 86) 3109\n",
      "Sample string (Document 86) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
      "\n",
      "Processing file stories\\088.txt\n",
      "Data size (Characters) (Document 87) 1365\n",
      "Sample string (Document 87) ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
      "\n",
      "Processing file stories\\089.txt\n",
      "Data size (Characters) (Document 88) 4538\n",
      "Sample string (Document 88) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
      "\n",
      "Processing file stories\\090.txt\n",
      "Data size (Characters) (Document 89) 345\n",
      "Sample string (Document 89) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
      "\n",
      "Processing file stories\\091.txt\n",
      "Data size (Characters) (Document 90) 5460\n",
      "Sample string (Document 90) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
      "\n",
      "Processing file stories\\092.txt\n",
      "Data size (Characters) (Document 91) 6854\n",
      "Sample string (Document 91) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
      "\n",
      "Processing file stories\\093.txt\n",
      "Data size (Characters) (Document 92) 2314\n",
      "Sample string (Document 92) ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
      "\n",
      "Processing file stories\\094.txt\n",
      "Data size (Characters) (Document 93) 1706\n",
      "Sample string (Document 93) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
      "\n",
      "Processing file stories\\095.txt\n",
      "Data size (Characters) (Document 94) 3229\n",
      "Sample string (Document 94) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
      "\n",
      "Processing file stories\\096.txt\n",
      "Data size (Characters) (Document 95) 4954\n",
      "Sample string (Document 95) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
      "\n",
      "Processing file stories\\097.txt\n",
      "Data size (Characters) (Document 96) 5732\n",
      "Sample string (Document 96) ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
      "\n",
      "Processing file stories\\098.txt\n",
      "Data size (Characters) (Document 97) 4334\n",
      "Sample string (Document 97) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
      "\n",
      "Processing file stories\\099.txt\n",
      "Data size (Characters) (Document 98) 7090\n",
      "Sample string (Document 98) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
      "\n",
      "Processing file stories\\100.txt\n",
      "Data size (Characters) (Document 99) 1007\n",
      "Sample string (Document 99) ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the text lowercase\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Breaking the text into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Creates a list of lists with the bigrams (outer loop different stories)\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries (Bigrams)\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "\n",
    "* `dictionary`: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449177 Characters found.\n",
      "Most common words (+UNK) [('e ', 15229), ('he', 15164), (' t', 13443), ('th', 13076), ('d ', 10687)]\n",
      "Least common words (+UNK) [('bj', 1), ('ii', 1), ('i?', 1), ('z ', 1), ('c.', 1), ('\"k', 1), ('pw', 1), ('f?', 1), (' z', 1), ('xq', 1), ('nm', 1), ('m?', 1), ('\\t\"', 1), ('\\tw', 1), ('tz', 1)]\n",
      "Sample data [15, 28, 86, 23, 3, 95, 74, 11, 2, 16]\n",
      "Sample data [22, 156, 25, 37, 82, 185, 43, 9, 90, 19]\n",
      "Vocabulary:  544\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the RNN. More specifically the generator breaks a given sequence of words into `batch_size` segments. We also maintain a cursor for each segment. So whenever we create a batch of data, we sample one item from each segment and update the cursor of each segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\te  (1), \tki (131), \t d (48), \t w (11), \tbe (70), \n",
      "\tOutput:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\tOutput:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\tOutput:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\tOutput:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tl, (257), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tbe (70), \n",
      "\tOutput:\n",
      "\tki (131), \t d (48), \t w (11), \tbe (70), \tau (195), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM, LSTM with Peepholes and GRUs\n",
    "\n",
    "* A LSTM has 5 main components\n",
    "  * Cell state, Hidden state, Input gate, Forget gate, Output gate\n",
    "* A LSTM with peephole connections\n",
    "  * Introduces several new sets of weights that connects the cell state to the gates\n",
    "* A GRU has 3 main components\n",
    "  * Hidden state, Reset gate and a Update gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "\n",
    "Here we define several hyperparameters and are very similar to the ones we defined in Chapter 6. However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 64\n",
    "num_unrollings = 50\n",
    "dropout = 0.2\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "valid_inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "valid_labels = tf.compat.v1.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.compat.v1.placeholder(tf.float32, shape=[1, vocabulary_size])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters and Cell Computation\n",
    "\n",
    "We define parameters and cell computation functions for all the different variants (LSTM, LSTM with peepholes and GRUs). **Make sure you only run a single cell within this section (either the LSTM/ LSTM with peepholes or GRUs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LSTM\n",
    "\n",
    "Here we define the parameters and the cell computation function for a standard LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.compat.v1.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "algorithm = 'lstm'\n",
    "filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTMs with Peephole Connections\n",
    "\n",
    "We define the parameters and cell computation for a LSTM with peepholes. Note that we are using diagonal peephole connections (for more details refer the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Parameters:\n",
    "# # Input gate: input, previous output, and bias.\n",
    "# ix = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# im = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# ic = tf.Variable(tf.compat.v1.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "# ib = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# # Forget gate: input, previous output, and bias.\n",
    "# fx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# fm = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# fc = tf.Variable(tf.compat.v1.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "# fb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "# # Memory cell: input, state and bias.                             \n",
    "# cx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# cm = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# cb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0,0.01))\n",
    "# # Output gate: input, previous output, and bias.\n",
    "# ox = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# om = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# oc = tf.Variable(tf.compat.v1.truncated_normal([1,num_nodes], stddev=0.01))\n",
    "# ob = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0,0.01))\n",
    "\n",
    "# # Softmax Classifier weights and biases.\n",
    "# w = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "# b = tf.Variable(tf.compat.v1.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# # Variables saving state across unrollings.\n",
    "# saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "# saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "# saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# saved_test_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "# saved_test_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "\n",
    "# algorithm = 'lstm_peephole'\n",
    "# filename_to_save = algorithm + filename_extension +'.csv'\n",
    "# # Definition of the cell computation.\n",
    "# def lstm_with_peephole_cell(i, o, state):\n",
    "#     '''\n",
    "#     LSTM with peephole connections\n",
    "#     Our implementation for peepholes is based on \n",
    "#     https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf    \n",
    "#     '''\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + state*ic + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + state*fc + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + state*oc + tf.matmul(o, om) + ob)\n",
    "\n",
    "#     return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "Finally we define the parameters and cell computations for the GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Parameters:\n",
    "# # Reset gate: input, previous output, and bias.\n",
    "# rx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# rh = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# rb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# # Hidden State: input, previous output, and bias.\n",
    "# hx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# hh = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# hb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# # Update gate: input, previous output, and bias.\n",
    "# zx = tf.Variable(tf.compat.v1.truncated_normal([vocabulary_size, num_nodes], stddev=0.01))\n",
    "# zh = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, num_nodes], stddev=0.01))\n",
    "# zb = tf.Variable(tf.compat.v1.random_uniform([1, num_nodes],0.0, 0.01))\n",
    "\n",
    "# # Softmax Classifier weights and biases.\n",
    "# w = tf.Variable(tf.compat.v1.truncated_normal([num_nodes, vocabulary_size], stddev=0.01))\n",
    "# b = tf.Variable(tf.compat.v1.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "# # Variables saving state across unrollings.\n",
    "# saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "# saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "# saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False)\n",
    "\n",
    "# algorithm = 'gru'\n",
    "# filename_to_save = algorithm + filename_extension +'.csv'\n",
    "\n",
    "# # Definition of the cell computation.\n",
    "# def gru_cell(i, o):\n",
    "#     \"\"\"Create a GRU cell.\"\"\"\n",
    "#     reset_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rh) + rb)\n",
    "#     h_tilde = tf.tanh(tf.matmul(i,hx) + tf.matmul(reset_gate * o, hh) + hb)\n",
    "#     z = tf.sigmoid(tf.matmul(i,zx) + tf.matmul(o, zh) + zb)\n",
    "#     h = (1-z)*o + z*h_tilde\n",
    "    \n",
    "#     return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM/GRU/LSTM-Peephole Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================================================\n",
    "# #Training related inference logic\n",
    "\n",
    "# # Keeps the calculated state outputs in all the unrollings\n",
    "# # Used to calculate loss\n",
    "# outputs = list()\n",
    "\n",
    "# # These two python variables are iteratively updated\n",
    "# # at each step of unrolling\n",
    "# output = saved_output\n",
    "# if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "#   state = saved_state\n",
    "\n",
    "# # Compute the hidden state (output) and cell state (state)\n",
    "# # recursively for all the steps in unrolling\n",
    "# # Note: there is no cell state for GRUs\n",
    "# for i in train_inputs:\n",
    "#     if algorithm=='lstm':\n",
    "#       output, state = lstm_cell(i, output, state)\n",
    "#       train_state_update_ops = [saved_output.assign(output),\n",
    "#                                 saved_state.assign(state)]\n",
    "#     elif algorithm=='lstm_peephole':\n",
    "#       output, state = lstm_with_peephole_cell(i, output, state)\n",
    "#       train_state_update_ops = [saved_output.assign(output),\n",
    "#                                 saved_state.assign(state)]\n",
    "#     elif algorithm=='gru':\n",
    "#       output = gru_cell(i, output)\n",
    "#       train_state_update_ops = [saved_output.assign(output)]\n",
    "        \n",
    "#     output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "#     # Append each computed output value\n",
    "#     outputs.append(output)\n",
    "\n",
    "# # calculate the score values\n",
    "# logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# # Compute predictions.\n",
    "# train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# # Compute training perplexity\n",
    "# train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# # ========================================================================\n",
    "# # Validation phase related inference logic\n",
    "\n",
    "# valid_output = saved_valid_output\n",
    "# if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "#   valid_state = saved_valid_state\n",
    "\n",
    "# # Compute the LSTM cell output for validation data\n",
    "# if algorithm=='lstm':\n",
    "#     valid_output, valid_state = lstm_cell(\n",
    "#         valid_inputs, saved_valid_output, saved_valid_state)\n",
    "#     valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "#                                 saved_valid_state.assign(valid_state)]\n",
    "    \n",
    "# elif algorithm=='lstm_peephole':\n",
    "#     valid_output, valid_state = lstm_with_peephole_cell(\n",
    "#         valid_inputs, saved_valid_output, saved_valid_state)\n",
    "#     valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "#                                 saved_valid_state.assign(valid_state)]\n",
    "# elif algorithm=='gru':\n",
    "#     valid_output = gru_cell(valid_inputs, valid_output)\n",
    "#     valid_state_update_ops = [saved_valid_output.assign(valid_output)]\n",
    "\n",
    "# valid_logits = tf.compat.v1.nn.xw_plus_b(valid_output, w, b)\n",
    "# # Make sure that the state variables are updated\n",
    "# # before moving on to the next iteration of generation\n",
    "# with tf.control_dependencies(valid_state_update_ops):\n",
    "#     valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# # Compute validation perplexity\n",
    "# valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.compat.v1.log(valid_prediction+1e-10))\n",
    "\n",
    "# # ========================================================================\n",
    "# # Testing phase related inference logic\n",
    "\n",
    "# # Compute the LSTM cell output for testing data\n",
    "# if algorithm=='lstm':\n",
    "#   test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "#   test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "#                             saved_test_state.assign(test_state)]\n",
    "# elif algorithm=='lstm_peephole':\n",
    "#   test_output, test_state = lstm_with_peephole_cell(test_input, saved_test_output, saved_test_state)\n",
    "#   test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "#                             saved_test_state.assign(test_state)]\n",
    "# elif algorithm=='gru':\n",
    "#   test_output = gru_cell(test_input, saved_test_output)\n",
    "#   test_state_update_ops = [saved_test_output.assign(test_output)]\n",
    "\n",
    "# # Make sure that the state variables are updated\n",
    "# # before moving on to the next iteration of generation\n",
    "# with tf.control_dependencies(test_state_update_ops):\n",
    "#     test_prediction = tf.nn.softmax(tf.compat.v1.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "if algorithm=='lstm':\n",
    "  state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "# Note: there is no cell state for GRUs\n",
    "for i in train_inputs:\n",
    "    if algorithm=='lstm':\n",
    "      output, state = lstm_cell(i, output, state)\n",
    "      train_state_update_ops = [saved_output.assign(output),\n",
    "                                saved_state.assign(state)]\n",
    "        \n",
    "    # output = tf.nn.dropout(output,keep_prob=1.0-dropout) ################# have to consider this dropout ###################\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.compat.v1.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "valid_output = saved_valid_output\n",
    "if algorithm=='lstm' :\n",
    "  valid_state = saved_valid_state\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "if algorithm=='lstm':\n",
    "    valid_output, valid_state = lstm_cell(\n",
    "        valid_inputs, saved_valid_output, saved_valid_state)\n",
    "    valid_state_update_ops = [saved_valid_output.assign(valid_output),\n",
    "                                saved_valid_state.assign(valid_state)]\n",
    "\n",
    "valid_logits = tf.compat.v1.nn.xw_plus_b(valid_output, w, b)\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(valid_state_update_ops):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.compat.v1.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "if algorithm=='lstm':\n",
    "  test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "  test_state_update_ops = [saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies(test_state_update_ops):\n",
    "    test_prediction = tf.nn.softmax(tf.compat.v1.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "We calculate the training loss of the LSTM here. It's a typical cross entropy loss calculated over all the scores we obtained for training data (`loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies(train_state_update_ops):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch). But since GRU doesn't have a cell state we have a conditioned reset_state ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if algorithm=='lstm' or algorithm=='lstm_peephole':\n",
    "#     # Reset train state\n",
    "#     reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "#                           tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "#     reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "#                           tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "#     # Reset test state. We use imputations in the test state reset\n",
    "#     reset_test_state = tf.group(\n",
    "#         saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01)),\n",
    "#         saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.01)))\n",
    "    \n",
    "# elif algorithm=='gru':\n",
    "#     # Reset train state\n",
    "#     reset_train_state = [tf.assign(saved_output, tf.zeros([batch_size, num_nodes]))]\n",
    "\n",
    "#     # Reset valid state\n",
    "#     reset_valid_state = [tf.assign(saved_valid_output, tf.zeros([1, num_nodes]))]\n",
    "    \n",
    "#     # Reset test state. We use imputations in the test state reset\n",
    "#     reset_test_state = [saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.01))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm=='lstm':\n",
    "    # Reset train state\n",
    "    reset_train_state = tf.group(tf.compat.v1.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.compat.v1.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "    reset_valid_state = tf.group(tf.compat.v1.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.compat.v1.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    # Reset test state. We use imputations in the test state reset\n",
    "    reset_test_state = tf.group(\n",
    "        saved_test_output.assign(tf.compat.v1.random_normal([1, num_nodes],stddev=0.01)),\n",
    "        saved_test_state.assign(tf.compat.v1.random_normal([1, num_nodes],stddev=0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using the Adam optimizer as it is one of the best optimizers out there. Furthermore we use gradient clipping to prevent any gradient explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for decaying learning rate\n",
    "gstep = tf.Variable(0, trainable=False)\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.compat.v1.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.compat.v1.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(tf_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "\n",
    "Here we train the model on the available data and generate text using the trained model for several steps. From each document we extract text for `steps_per_document` steps to train the model on. We also report the train perplexity at the end of each step. Finally we test the model by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "(11).(52).(25).(34).(64).(5).(43).(39).(74).(68).\n",
      "Average loss at step 1: 4.316420\n",
      "\tPerplexity at step 1: 74.919956\n",
      "\n",
      "Valid Perplexity: 58.71\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  his was that hing his hime the said, and the\n",
      "sed the was to he was here and his he the was one the said, the was there the was that, and\n",
      "him ing, and was he to all him, and that, the said, and his ing the the said, the said his the will her the was one the said, but ther he to and and and as the saw there the was the said, and the was to him then the was that then his that his so his to he the the was to and and shen his the said, and the said that the that his shen the was ing, then his that he thoughnto that they himself the was the said the wite to he was to him him, and here and said, and said, and him, and the was ther, the the saide the saide to to her on and and witk his her her the was one that the was was her, and, and he was that, the was to he was the was to and he that then his his ing her her hing, and the said, and her the was that he was to he was to that the said, the her the seve him, but the sed the\n",
      "sed will him hime the saide to his to he himself, and that his wash th\n",
      "====================================================================\n",
      "\n",
      "(85).(9).(28).(82).(96).(14).(11).(29).(30).(39).\n",
      "Average loss at step 2: 3.032063\n",
      "\tPerplexity at step 2: 20.739968\n",
      "\n",
      "Valid Perplexity: 54.61\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t lf lay and them, and bring to dring to they saw the duck andever ling to had there and then were the children it.  there if the children, and the chrester, and were that they her, and were had nown.\n",
      "\n",
      "the children aboth, ther hearth you cook, and not now, the church, and then were the church, and the children it, and went here to had to drink it, and the servants heartill not the cook, and not, with the rominger, they had leaver.  the cook to drina, they said, they said the servants herself, the second the children.  but the water, and that\n",
      "the cook asked, the rored the duck and there to dring to the cook, will.  they are i will ina, the cook and the duck andever, the cook to eaver leave, and there when the dund them, when went the children the\n",
      "sentere ina, to the\n",
      "childeen, and went hearth the come will.  then the cook and went to you will nother, it they had it, and there if the duck away a chandelied and that the three servants to dring to her, and were to dring to her, and went hease, \n",
      "====================================================================\n",
      "\n",
      "(95).(19).(44).(73).(92).(7).(98).(9).(85).(99).\n",
      "Average loss at step 3: 2.807090\n",
      "\tPerplexity at step 3: 16.561654\n",
      "\n",
      "Valid Perplexity: 85.62\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  care you much dowry the a\n",
      "broom-mair UNKather.\" UNKerhaps a\n",
      "broom-maker?\" \"yes, that's something better.\" \"something a trie.\" \"a husbandmanUNK\"\n",
      "UNKomanUNKer?\" \"something better.\" \"a joiner?\" \"something better.\" \"a husband a smit, and-andmother ine triecher.\" \"something better.\" \"a hunding, \"many thanks, and if the trade?\"\n",
      "\" UNKer is you muthing better.\" \"a smitch thank, whater and a palleUNK\" \"something better.\" \"ser is it not is your handflf, i ame traiden her mother high-and many the kings am, and a\n",
      "broom-maker?\"\n",
      "\"yes, and is faUNKers, \"yes, that's what thim, and is your handful, i hand a pan-pand mighty, andful migchand will is a fing of\n",
      "siUNKer?\" \"something better.\" \"a smit?\" \"something better.\" \"a husbandman?\"\n",
      "\"somethings and aethis mout of\n",
      "ther?\"\n",
      "\"something better.\" \"a husbandman?\"\n",
      "\"something better.\" \"a smit?\" \"something better.\" \"son thing better.\" \"a smitUNK\"\n",
      "\"something better.\" UNKerhaps a pif-paf-poltrie.\" \"many trank, wher i am, is\n",
      "it?\" \"something better.\" \"a joiner?\" \"something better\n",
      "====================================================================\n",
      "\n",
      "(32).(50).(26).(74).(83).(13).(39).(70).(78).(91).\n",
      "Average loss at step 4: 2.490316\n",
      "\tPerplexity at step 4: 12.065086\n",
      "\n",
      "Valid Perplexity: 35.68\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ating the comiller, who said to the miller, and she was said, said, \"i wout he three witchering for that so as it, whe was the witch he self, and she meal so one me, the said them and fore it.\" the said the wast of with\n",
      "hem founts and three ones the moreered it.  the human she was and\n",
      "toger that shead broom, and that said, \"so them for to her again, and the mill, and not they came one of them and as as the morning he wast.  i have she will beatings and said, \"ked\n",
      "the was not to the mecame for ther, and should not to them and three with one again, and he sant, and hear the manched of her huntsman she would nother thought\n",
      "iting and her and have said, \"keep it.\" an to methe miller with there and said the wast of the will they came, and he said the dearth.\n",
      "then said, \"i his one, and said the wast on he self of the salad for to the could not to that stillown he was that the was not into pre that they cam no was deard to three as a forgaid, \"the said, and she said to the coundearly to beating \n",
      "====================================================================\n",
      "\n",
      "(98).(88).(24).(27).(26).(77).(67).(7).(42).(97).\n",
      "Average loss at step 5: 2.317483\n",
      "\tPerplexity at step 5: 10.150092\n",
      "\n",
      "Valid Perplexity: 37.38\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t r the\n",
      "wort, anstonder the priecess, and her same, it is the\n",
      "king's daughter.\n",
      "\n",
      "he was said to the tragon.  so they mine.\"\n",
      "\n",
      "then then the king, the king's dre and hear together again, and them agained the\n",
      "seach of the\n",
      "gries of a don an each of\n",
      "them. eceived half a kingdom, and they lived with his her the man,ed the waid to themsest has ster thuntsman shouldered he his vervantsmit, i will give to each of you, and\n",
      "said, \"you and the pried, anted had not sese she took his brothers and show to the would him againly time the king's peachers, and it stopched, the king's daughter, \"and said,\n",
      "\"you a god, and said, \"what would all they are the fire of to before to be at instieces only thundinto the whold as long as and said to his be at his greaself and ally his\n",
      "than she was seedle, and the fire, and the shole and all gree of in betting the soon all the peach of you shall of in she sewn own togethis decision, and said his and the king el, so that her ther time the king, they all of you have her, an\n",
      "====================================================================\n",
      "\n",
      "(25).(68).(23).(24).(16).(42).(49).(98).(41).(15).\n",
      "Average loss at step 6: 2.042544\n",
      "\tPerplexity at step 6: 7.710200\n",
      "\n",
      "Valid Perplexity: 29.08\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  was it was\n",
      "fored the shoe, for the golden sto the pigeons, horn stilled the king's son.  then therelder, she went one and that she and\n",
      "the maid no went that her shoe, and her maiden if it, and ped sittle baced, it the pressed to the peried her hand and ber me the two on her she was and maiden in the\n",
      "tree wither home this and they came of her the pigeon-houst an one on a little that she had to him, for she tooking-glass, on the shoe,\n",
      "     that the tree been the two sisters and peep, and the charUNKers took her, they came and they came, and said, and took the tree was the king's son, and said the true by into the right, and then the pallen into the falted to her facher mouth.  and the tree blood, and the younger to to ges into the pigeouse on her stood and ran and she was to the true the pigeons\n",
      "posefore the traven, they could not he was not too small for\n",
      "her.  when he wanted to get into the there for herself it one in his hor could, and the king's son with her the eyes shoe, however, and \n",
      "====================================================================\n",
      "\n",
      "(12).(10).(5).(22).(58).(78).(96).(24).(92).(74).\n",
      "Average loss at step 7: 1.703954\n",
      "\tPerplexity at step 7: 5.495634\n",
      "\n",
      "Valid Perplexity: 48.58\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  bearned to do it to he well, i had goman, and who was quite was to doin the waster, beiut one days, but you cave inded into the king's son behindt to and who wasand\n",
      "saild,, and have you are wite she have it.\" and\n",
      "then the table whithere whent he had to the more to the ked told in his form.\" then the king the knapss her mother and put the devil\n",
      "said, \"shallooked arounter your, if you have your hell on his himself, and that he hans was he was no my belonce mornt to sold before the dove.\" but when the have\n",
      "he had to\n",
      "play, and the king was so delitted the bown to please the your led the king's\n",
      "daughter, and wine the about him ho have him and asked, and when he was not his a would trimme and father the devil was soldied\n",
      "himself, which had the you come of it.  but hith he sarn he would not him to hold water that the dange.  when the old lake into the deare that she white, should he was in hears, and nate\n",
      "himself, she was not his dance beent back again fullying gave him and you core of which b\n",
      "====================================================================\n",
      "\n",
      "(47).(13).(40).(10).(97).(42).(51).(96).(62).(66).\n",
      "Average loss at step 8: 1.649118\n",
      "\tPerplexity at step 8: 5.202392\n",
      "\n",
      "Valid Perplexity: 33.91\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t he gave then the old not well be and sound his cart, and turn the bander anything to the broth, her let went out on the thre, and began to go her hand, then he was the bailiff hand on the was lighter she have, the others, and now went and asked from therce, and was lighte, and beent to slew\n",
      "there back, and not know what to received fore therefor the the bailiff, and was away.\n",
      "\n",
      "the head-man that it, had head-man had she wast on, and heard thishe willed thather on, i have off was to see and his own her together, he said they were now wanted to not out, and the beautif you came to tow one, the ways away, and that he than the youth will tood for the\n",
      "hornly all hear of the look coat.  i his said to the brother was servants to good fours and the others, and head the bailiff, and soon the bailiff, said he, and waid not se could and had by to before on that he was hearned to take him, she have, i will take\n",
      "othed forething her, and when the paily the youth to to get ife enough, and they had shove\n",
      "====================================================================\n",
      "\n",
      "(72).(33).(80).(98).(66).(47).(40).(57).(87).(43).\n",
      "Average loss at step 9: 1.665186\n",
      "\tPerplexity at step 9: 5.286658\n",
      "\n",
      "Valid Perplexity: 62.83\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t cks.  she had ever heart there more beforeUNK much the miller,\n",
      "nother and asked herself in\n",
      "ortuney, in that with the mallower to his lightend, she shall be\n",
      "humblg, and she spiried her a life was stild, shall, is she night and anothisher unged the\n",
      "fore ayou all as into the\n",
      "herestrythe man.  and girl that she then, and then she marnith ang cir, ind nappar.  the straid that, when that the your name, and said, your ave still her, then the toeen was a young loved you shall have too yout ever, and\n",
      "him of the likewe the anumng, if she replinappy , you had the gill ais the ever himself or two\n",
      "on the third\n",
      "travelther the devil had told, said the girl was sheep, now no tim you manseng i daughter becarewas no, and secoght room the meade, and then gress queen ind\n",
      "worn me.  that you i harl with the miller, and this was a propenes was brought to them good, and for the mill you.  but son said, i'lill give your not into the miller's boir her, and this his name the peep.\n",
      "no shall you manikin it was jumpin\n",
      "====================================================================\n",
      "\n",
      "(90).(37).(14).(31).(74).(61).(5).(81).(30).(55).\n",
      "Average loss at step 10: 1.933914\n",
      "\tPerplexity at step 10: 6.916529\n",
      "\n",
      "Valid Perplexity: 39.23\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t d of six, which were shousenget this were was of gold to the sack, should hat a hen cause, but is not travelve them to a food hame, home in a little said he his stronges.  oh, yes, there it carried, i will give if got to be the king and beating of this well up in the sack, and sack, he came to takenhis can stairs, and with the cire to the took him.  then shoemney his sitter, who way not true.  then the rack that man who was now to his sack, in the air.  and when he saw that should vert, and they coosed the her.  then the six will no clept the blother gold it wor found the shiles arger, this long as little the shouldeer that a longer of his eyes were was and deliven with the coor help into\n",
      "that is away well up, th had that and said to him and carry the old streng all the blower caused the six were block to the thinger, the roay frotherver him.  i don't trim yes me.\" and\n",
      "then it the other godfather, and someted how must of the strong one what all that a longer was well be man.  the blower \n",
      "====================================================================\n",
      "\n",
      "(36).(79).(29).(53).(41).(70).(65).(91).(57).(71).\n",
      "Average loss at step 11: 1.719608\n",
      "\tPerplexity at step 11: 5.582341\n",
      "\n",
      "Valid Perplexity: 44.62\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ding at last of he wout to the her, and the kill thrusent to the old woman to his huntsmaid to lar,\n",
      "                then diece, and the kind threning to peace.\"  then he and\n",
      "drung on youth they arrifield mother to there\n",
      "stook, and when she was a little to three boy the daughter gave you the fishe\n",
      "man into the coals.\"\n",
      "\n",
      "\"many thisher and the waller, and she was\n",
      "a little brown beaut of alfeas and said, and said, beautif you seen her into the cast tone this the wall\n",
      "one again a she had grown quike for her by the prinand then the others was hiled to she had grown queen, and then he said them into the othere.  then they had him and were bried her to the took to the old woman, and when the wate werson to went on the witchan one quant, and then he brought him with go took the white likewise.\n",
      "\n",
      "\"i am the fished, which you set the\n",
      "fishermadelied the king find, and she\n",
      "fishall togethere,\n",
      "           the fishes you aoner, and then they the was to one again, and the sottleest, and had gat tas hanself b\n",
      "====================================================================\n",
      "\n",
      "(22).(30).(19).(83).(45).(99).(8).(73).(4).(1).\n",
      "Average loss at step 12: 2.193436\n",
      "\tPerplexity at step 12: 8.965964\n",
      "\n",
      "Valid Perplexity: 25.24\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ful mercury it at the queen become and she little was it, and said the she crided her the children as a was to person, and the firsts of peather carry to pain, and if your there was of deather.  then the same that he will bed to her the child first was\n",
      "forcend conded to the doom ther, the\n",
      "queen beautiful some of the golden was about and the queen about her they the queen belong the child coming towere the childread from the forgother.  but the king went on the stone, the deving bead the third in the might shoved and been were in the morden the virgs into the forest to she could not believe to the changer to the she could not believe to the child of a dearth the devil in his paterness child bird in his to ber and took the country, and as\n",
      "confind out that the\n",
      "horse, and he said, and then shey will could not be deart, and the king\n",
      "was next morning, he was to be said on\n",
      "the wood once the king was sorrow, and i am long be are her to beautifull as wife but of the forget to eady that the\n",
      "virgin\n",
      "====================================================================\n",
      "\n",
      "(67).(81).(12).(60).(91).(71).(83).(30).(44).(73).\n",
      "Average loss at step 13: 1.834091\n",
      "\tPerplexity at step 13: 6.259442\n",
      "\n",
      "Valid Perplexity: 36.72\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      "\t ts\n",
      "her as got in\n",
      "its\n",
      "have with himself and busor about to her, it dispnoit.  the sto had ground.  she played up to the your\n",
      "ways and you thank into a second the this, i will give you canry, and he sid that the fine of when she father, and said, \"i will standing in\n",
      "his sing took thanks, and a ligk, but, he longed he for thought, when he had doesn sever you seatime, and the mittened.  i must taken by the fight and,\n",
      "\"inhe want woman.\n",
      "\n",
      "have done you.  not?\n",
      "\"good home, the father land he the king all well be prought\n",
      "herself, and the restess, and whow a verlard as the royan ang, and cont ated with sother, when\n",
      "here all he saw that you somethird to be alle and which camethrown assed him, but\n",
      "as when the morning as he could not seve on the world.          i here then the birds, and the son that that was little you are, then the father gavather and horned trouble, and i can the wolf be the world,\" said the kate, you said the black to a how, and the spirited at retch came falther, and\n",
      "that with th\n",
      "====================================================================\n",
      "\n",
      "(87).(70).(49).(63).(99).(5).(76).(74).(54).(27).\n",
      "Average loss at step 14: 1.821900\n",
      "\tPerplexity at step 14: 6.183597\n",
      "\n",
      "Valid Perplexity: 32.67\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t there\n",
      "gan was a birder, and then hor the couse, where the sat of an whill have not were in the cat, what all will\n",
      "was brothing.  they saw that able with you have became, and the sat down at his father, hereupon and how, for to him.  as if the whold great dead.  then they had now treach of and mill.  the wat, and as she heard of your eyes, and it them are worh me.\n",
      "\n",
      "and to be himself a follower, i will way, and said, they were world, said his father, it hat it was full be brought, and the king saw that her willow-which however, which was he cried, so monce.  there the flew his face, and sat down, ord the room with the mother to eat then.  ah, said, \"oh, you are of still, and they have heared, and well from his fald had dise to greate asked to take his devil spinces what down, and told he in\n",
      "read asked, and when it wanted to his how of their pinents and as he had better, and said, than he hadsent and began to him, out of puh, his head than, he she would soon to say which is wery site.  that\n",
      "====================================================================\n",
      "\n",
      "(42).(37).(17).(13).(92).(65).(63).(33).(95)."
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "docs_per_step = 10\n",
    "valid_summary = 1\n",
    "train_doc_count = num_files\n",
    "\n",
    "session = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# Capture the behavior of train/valid perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "# Initializing variables\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity\n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity\n",
    "        \n",
    "        # shows the training progress\n",
    "        print('(%d).'%di,end='') \n",
    "        \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "        session.run(reset_train_state) # resetting hidden state for each document\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses  \n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        test_word[0,data_list[np.random.randint(0,num_files)][np.random.randint(0,100)]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})            \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write the perplexity data to a CSV\n",
    "\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Part D – Evaluation and Testing</h4> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Part E - Evaluate results</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ecb043c468c934e7a808d478000b9a855468e7cce0908c449a2c1291f7fe6d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
